---
taskId: oolong-17000211
score: 1.0
iterations: 14
wallTimeMs: 189602
answerType: ANSWER_TYPE.COMPARISON
taskGroup: TASK_TYPE.COMPARISON
answer: "Answer: description and abstract concept is less common than human being"
expected: "less common than"
error: null
patterns:
  - format-discovery
  - chunking
  - delegation-llm
  - parallel-fanout
  - prompt-crafting
  - incremental-refinement
  - self-correction
  - verification
failureMode: null
verdict: perfect
---

# Trajectory: oolong-17000211

## Task Summary

Compare frequency of two labels ('description and abstract concept' vs 'human being') in a 3182-question dataset (316K context). Expected: "less common than". Got: "description and abstract concept is less common than human being". Score: 1.0 (perfect).

## Control Flow

```
iter 1  EXPLORE     probe context type, length, inspect first 500 chars
iter 2  EXPLORE     parse into lines, count total lines (3186)
iter 3  EXTRACT     extract questions using regex, get 3182 questions
iter 4  PLAN        create batches (64 × 50), design classification prompt
iter 5  DELEGATE    classify all batches via llm() parallel fanout
iter 6  EXTRACT     aggregate results, get 3349 total (overcounted by 167)
iter 7  DELEGATE    retry with stricter prompt emphasizing one-per-question
iter 8  EXTRACT     aggregate attempt 2, get 3224 total (off by 42)
iter 9  DELEGATE    retry with smaller batches (80 × 40) and stricter prompt
iter 10 EXTRACT     aggregate attempt 3, get 3238 total (off by 56), batches miscounting
iter 11 DELEGATE    switch to enumeration format for exact control
iter 12 EXTRACT     parse enumerated results, count DA=352, HU=416 (3181/3182)
iter 13 VERIFY      verify comparison logic, confirm DA < HU
iter 14 RETURN      return("description and abstract concept is less common than human being")
```

## Phase Analysis

### Phase 1: Exploration and Extraction (iter 1-3)
**Strategy:** Standard data probing followed by regex-based question extraction
**Effectiveness:** Excellent. Quickly identified plain-text format, parsed header structure, and extracted all 3182 questions from 3186 lines using `.match(/Instance:\s*(.+)/)` regex.
**Assessment:** Efficient initial exploration. No wasted iterations.

### Phase 2: First Classification Attempt (iter 4-6)
**Strategy:** Batch classification with llm() parallel fanout (64 batches of 50 questions)
**Result:** Classified questions but got 3349 total (167 overcounts)
**Assessment:** Good instinct to use batching and parallelization. However, the prompt wasn't strict enough about one-category-per-question, leading to double-counting. The agent correctly identified this as a problem.

### Phase 3: Prompt Refinement (iter 7-8)
**Strategy:** Retry with "stricter prompt" emphasizing "EXACTLY ONE category per question"
**Result:** Got 3224 total (42 overcounts) - better but still inaccurate
**Assessment:** Shows good iterative refinement and self-correction. The agent recognized the continued overcounting and tried again.

### Phase 4: Smaller Batches (iter 9-10)
**Strategy:** Reduce batch size from 50 to 40, add "CRITICAL: Your category counts MUST sum to exactly the number of questions provided"
**Result:** Got 3238 total (56 overcounts). Batch sums revealed individual batches were miscounting (47, 45, 46, 50 instead of 40)
**Assessment:** Excellent debugging - the agent logged batch sums and discovered the root issue wasn't aggregation but individual batch miscounts. This insight led to the next strategy.

### Phase 5: Enumeration Format (iter 11-12)
**Strategy:** Switch from "return JSON counts" to "enumerate each question's category on one line" format. This gives the agent exact control - the LLM classifies, the agent counts.
**Result:** Parsed 3181/3182 questions with counts DA=352, HU=416
**Assessment:** **Key success factor.** This format eliminated the LLM counting unreliability. The agent correctly reasoned that classification is easy for LLMs but counting is error-prone, so it decoupled the two tasks.

### Phase 6: Verification and Return (iter 13-14)
**Strategy:** Explicit verification of comparison logic before returning
**Result:** Verified 352 < 416, confirmed answer, returned correct result
**Assessment:** Good practice. The verification was simple but deliberate.

## Success Factors

1. **Adaptive strategy:** When initial classification attempts produced overcounts, the agent didn't get stuck. It tried three different approaches (stricter prompts, smaller batches, enumeration format) showing strong multi-strategy problem-solving.

2. **Root cause diagnosis:** In iteration 10, the agent logged `batchSums` and discovered individual batches were miscounting. This debugging insight directly led to the successful enumeration strategy - it identified that LLM counting was the issue, not aggregation logic.

3. **Task decomposition:** The final enumeration approach (iter 11-12) succeeded because it decomposed the task into what each component does well:
   - LLM: classify individual questions (high accuracy)
   - Agent code: count occurrences with regex (perfect accuracy)

   This is the canonical pattern for handling LLM unreliability in numeric tasks.

4. **Parallel delegation efficiency:** Used `Promise.all()` for all llm() calls, processing 64-80 batches in parallel. Without this, the task would have been much slower.

5. **Prompt engineering persistence:** The agent refined prompts three times (basic → strict → very strict) before recognizing the format itself was the issue. This shows good incremental refinement without premature abandonment.

6. **Clean verification:** Explicitly verified the comparison logic in iteration 13 with clear logging ("352 < 416" → "less common than") before returning. This catch-before-return pattern prevents silly last-minute errors.

## What Would Have Helped

1. **Documentation on LLM counting unreliability:** The agent discovered through trial that LLMs miscount when asked to "classify and count." A cookbook pattern showing the enumeration-then-regex-count approach would have saved iterations 5-10 (5 iterations).

2. **Structured output plugin:** A built-in tool for "map this array through llm() and parse structured output" would make the enumeration pattern trivial. Current implementation required manual Promise.all() + regex parsing.

3. **Sanity check heuristic:** After iteration 6 got 3349 instead of 3182, a simple alert ("total ≠ expected, likely overcounting") would have signaled the need for format change earlier.

4. **Comparison task template:** For frequency comparison tasks, a template showing "extract → batch classify with enumeration → count with code → compare" would have streamlined the approach.

5. **Nothing critical:** This trajectory is already very strong. The agent self-corrected efficiently and reached the right answer with good reasoning. The suggestions above would optimize iteration count but aren't necessary for success.
