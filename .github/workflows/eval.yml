name: Eval

on:
  workflow_dispatch:
    inputs:
      benchmark:
        description: Benchmark to run
        required: true
        type: choice
        options:
          - oolong
          - s-niah
          - arc
      model:
        description: "Model (provider/id, e.g. google/gemini-2.5-flash)"
        required: true
        default: qwen/qwen3-coder
      max-tasks:
        description: Max tasks (blank = all)
        required: false
      max-iterations:
        description: Max REPL iterations per task
        required: false
        default: "15"
      max-depth:
        description: Max recursion depth
        required: false
        default: "1"
      concurrency:
        description: Parallel tasks
        required: false
        default: "5"
      drivers:
        description: "Comma-separated driver plugins (e.g. no-tool-calls,one-block-per-iteration)"
        required: false
      app:
        description: "App plugin name (e.g. structured-data-aggregation)"
        required: false
      selected-problems:
        description: "ARC: comma-separated problem IDs (default: all)"
        required: false
      max-blocks-per-iteration:
        description: "Max code blocks per iteration (for single-block enforcement, use 1)"
        required: false
      analyze:
        description: Run trajectory analysis after eval
        type: boolean
        default: true

jobs:
  eval:
    runs-on: ubuntu-latest
    timeout-minutes: 180
    permissions:
      contents: read
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: 22
          cache: npm
      - run: npm ci

      - name: Cache OOLONG eval data
        if: inputs.benchmark == 'oolong'
        id: cache-oolong
        uses: actions/cache@v4
        with:
          path: eval/data/oolong
          key: oolong-eval-data-v1

      - name: Download dataset
        if: inputs.benchmark == 'oolong' && steps.cache-oolong.outputs.cache-hit != 'true'
        run: npx tsx eval/download.ts --from-release

      - name: Cache ARC eval data
        if: inputs.benchmark == 'arc'
        id: cache-arc
        uses: actions/cache@v4
        with:
          path: eval/data/arc
          key: arc-eval-data-v1

      - name: Download ARC dataset
        if: inputs.benchmark == 'arc' && steps.cache-arc.outputs.cache-hit != 'true'
        run: npx tsx eval/download.ts --dataset arc

      - name: Run eval
        env:
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
          NODE_OPTIONS: --max-old-space-size=4096
        run: |
          ARGS="--benchmark ${{ inputs.benchmark }} --model ${{ inputs.model }}"
          ARGS="$ARGS --max-iterations ${{ inputs.max-iterations }}"
          ARGS="$ARGS --max-depth ${{ inputs.max-depth }}"
          ARGS="$ARGS --concurrency ${{ inputs.concurrency }}"
          if [ -n "${{ inputs.max-tasks }}" ]; then
            ARGS="$ARGS --max-tasks ${{ inputs.max-tasks }}"
          fi
          if [ -n "${{ inputs.drivers }}" ]; then
            ARGS="$ARGS --drivers ${{ inputs.drivers }}"
          fi
          if [ -n "${{ inputs.app }}" ]; then
            ARGS="$ARGS --app ${{ inputs.app }}"
          fi
          if [ -n "${{ inputs.selected-problems }}" ]; then
            ARGS="$ARGS --selected-problems ${{ inputs.selected-problems }}"
          fi
          if [ -n "${{ inputs.max-blocks-per-iteration }}" ]; then
            ARGS="$ARGS --max-blocks-per-iteration ${{ inputs.max-blocks-per-iteration }}"
          fi
          npx tsx eval/run.ts $ARGS

      - name: Analyze results
        run: npx tsx eval/analyze.ts

      - name: Upload results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: eval-${{ inputs.benchmark }}-${{ github.run_number }}
          path: eval/results/
          retention-days: 90

  analyze:
    if: inputs.analyze == true
    needs: [eval]
    runs-on: ubuntu-latest
    timeout-minutes: 60
    permissions:
      contents: read
      id-token: write
    steps:
      - uses: actions/checkout@v4

      - name: Download eval results
        uses: actions/download-artifact@v4
        with:
          name: eval-${{ inputs.benchmark }}-${{ github.run_number }}
          path: eval/results/

      - name: Distill trajectories
        uses: anthropics/claude-code-action@v1
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          claude_args: '--allowedTools Bash Write Read Glob Grep Task'
          prompt: |
            You are distilling RLM eval trajectories into annotated documents.

            ## Setup
            1. Find the most recent .json file in eval/results/
            2. Read docs/TRAJECTORY_FORMAT.md — this is the canonical annotation format
            3. Run: mkdir -p eval/trajectory-analysis/trajectories

            ## Step 1: Enumerate all tasks
            Read the results JSON and extract every task. For each task, record:
            - taskId, answerType, score, iterations count
            - Classify outcome: score==1 → "perfect", 0<score<1 → "partial", score==0 → "wrong/timeout/error"
            Write the complete list (ALL tasks, no sampling or filtering) to
            eval/trajectory-analysis/sample.json as a JSON array:
            [{ taskId, answerType, outcome, score, iterations }, ...]

            ## Step 2: Annotate all trajectories
            For every task in the list, use the Task tool (model: sonnet) to annotate it.
            Launch up to 10 subagents at a time using run_in_background: true.
            After launching a batch of up to 10, wait for all of them to complete
            before launching the next batch. Continue until every task is annotated.

            Each sonnet agent needs:
            - The full raw trace data for its task (extract it from the results JSON
              and pass it in the prompt — do NOT make the agent re-read the large file)
            - The full text of docs/TRAJECTORY_FORMAT.md

            Each agent must:
            1. Read the raw trace ({reasoning, code[], output, error} per iteration)
            2. Classify each iteration into a phase (EXPLORE, EXTRACT, DELEGATE, VERIFY, ERROR, RETURN, etc.)
            3. Identify behavioral patterns (filtering, chunking, delegation-llm, jq-on-plaintext, etc.)
            4. Identify the failure mode if score < 1.0
            5. Write: Control Flow, Phase Analysis, Root Cause or Success Factors, What Would Have Helped
            6. Use YAML frontmatter with all required fields from the format spec
            7. Write the file to eval/trajectory-analysis/trajectories/{taskId}.md

            Be precise. Quote actual code and output from the trace. Don't speculate
            beyond what the trace evidence shows.

      - name: Upload trajectory analysis
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: trajectory-analysis-${{ inputs.benchmark }}-${{ github.run_number }}
          path: eval/trajectory-analysis/
          retention-days: 90
